# Copyright 2025 Thousand Brains Project
# Copyright 2023 Numenta Inc.
#
# Copyright may exist in Contributors' modifications
# and/or contributions to the work.
#
# Use of this source code is governed by the MIT
# license that can be found in the LICENSE file or at
# https://opensource.org/licenses/MIT.
"""
Data I/O, filesystem, and other utilities.
"""

import json
import os
from copy import deepcopy
from numbers import Number
from pathlib import Path
from typing import Any, Mapping, Optional

import numpy as np
import pandas as pd
import torch
from numpy.typing import ArrayLike

# Path settings - mirrors those in configs/common.py
DMC_ROOT_DIR = Path(os.environ.get("DMC_ROOT_DIR", "~/tbp/results/dmc")).expanduser()
DMC_PRETRAIN_DIR = DMC_ROOT_DIR / "pretrained_models"
DMC_RESULTS_DIR = DMC_ROOT_DIR / "results"
VISUALIZATION_RESULTS_DIR = DMC_ROOT_DIR / "visualizations"

# Root directory for output generated by figure scripts.
DMC_ANALYSIS_DIR = Path(
    os.environ.get("DMC_ANALYSIS_DIR", "~/tbp/results/dmc_analysis")
).expanduser()
DMC_ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)


def load_eval_stats(exp: os.PathLike) -> pd.DataFrame:
    """Load `eval_stats.csv`

    Convenience function for loading `eval_stats.csv` from a DMC experiment name.

    Args:
        exp (os.PathLike): Name of a DMC experiment, a directory containing
        `eval_stats.csv`, or a complete path to `eval_stats.csv`.

    Returns:
        pd.DataFrame: The loaded dataframe. Includes generated columns `episode` and
        `epoch`.
    """

    path = Path(exp).expanduser()

    if path.exists():
        # Case 1: Given a path to a csv file.
        if path.suffix.lower() == ".csv":
            df = pd.read_csv(exp, index_col=0)
        # Case 2: Given a path to a directory containing eval_stats.csv.
        elif (path / "eval_stats.csv").exists():
            df = pd.read_csv(path / "eval_stats.csv", index_col=0)
        else:
            raise FileNotFoundError(f"No eval_stats.csv found for {exp}")
    else:
        # Given a run name. Look in DMC folder.
        df = pd.read_csv(DMC_RESULTS_DIR / path / "eval_stats.csv", index_col=0)

    # Collect basic info, like number of LMs, objects, number of episodes, etc.
    n_lms = len(np.unique(df["lm_id"]))
    object_names = np.unique(df["primary_target_object"])
    n_objects = len(object_names)

    # Add 'episode' column.
    assert len(df) % n_lms == 0
    n_episodes = int(len(df) / n_lms)
    df["episode"] = np.repeat(np.arange(n_episodes), n_lms)

    # Add 'epoch' column.
    rows_per_epoch = n_objects * n_lms
    assert len(df) % rows_per_epoch == 0
    n_epochs = int(len(df) / rows_per_epoch)
    df["epoch"] = np.repeat(np.arange(n_epochs), rows_per_epoch)

    return df


def get_percent_correct(df: pd.DataFrame) -> float:
    """Get percent of correct object recognition for an `eval_stats` dataframe.

    Uses the 'primary_performance' column. Values 'correct' or 'correct_mlh' count
    as correct.

    """
    n_correct = df.primary_performance.str.startswith("correct").sum()
    return 100 * n_correct / len(df)


def describe_dict(data: Mapping, level: int = 0):
    """
    Recursively describe the contents of a nested dictionary. For visualizing the
    structure of detailed JSON stats. Can be removed when out of data exploration phase.

    Args:
        data (dict): The dictionary to describe.
        level (int): Current depth level in the nested dictionary.
    """
    if not isinstance(data, dict):
        print(f"{'  ' * level}- Not a dictionary: {type(data).__name__}")
        return

    for key in sorted(data.keys()):
        value = data[key]
        print(f"{'  ' * (level + 1)}'{key}': {type(value).__name__}")
        if isinstance(value, dict):
            # Recursively describe nested dictionaries
            describe_dict(value, level + 1)


class DetailedJSONStatsInterface:
    """Convenience interface to detailed JSON stats.

    This class is a dict-like interface to detailed JSON stats files that loads
    episodes one at a time. An episode can be loaded via `stats[episode_num]`
    (or, equivalently `stats.read_episode(episode_num)`), which takes about
    1.5 - 6.5 seconds per episode. If you plan on loading all episodes eventually,
    the most efficient method is to iterate over a `DetailedJSONStatsInterface`.

    Example:
        >>> stats = DetailedJSONStatsInterface("detailed_stats.json")
        >>> last_episode_data = stats[-1]  # Get data for the last episode.
        >>> # Iterate over all episodes.
        >>> for i, episode_data in enumerate(stats):
        ...     # Process episode data
        ...     pass
    """

    def __init__(self, path: os.PathLike):
        self._path = Path(path)
        self._index = None  # Just used to convert possibly negative indices

    @property
    def path(self) -> os.PathLike:
        return self._path

    def read_episode(self, episode: int) -> Mapping:
        self._check_initialized()
        assert np.isscalar(episode)
        episode = self._index[episode]
        with open(self._path, "r") as f:
            for i, line in enumerate(f):
                if i == episode:
                    return json.loads(line)[str(i)]

    def _check_initialized(self):
        if self._index is not None:
            return
        length = 0
        with open(self._path, "r") as f:
            length = sum(1 for _ in f)
        self._index = np.arange(length)

    def __iter__(self):
        with open(self._path, "r") as f:
            for i, line in enumerate(f):
                yield json.loads(line)[str(i)]

    def __len__(self) -> int:
        self._check_initialized()
        return len(self._index)

    def __getitem__(self, episode: int) -> Mapping:
        """Get the stats for a given episode.

        Args:
            episode (int): The episode number.

        Returns:
            Mapping: The stats for the episode.
        """
        return self.read_episode(episode)


class ObjectModel:
    def __init__(
        self,
        points: ArrayLike,
        translation: ArrayLike = (0, 0, 0),
        rgba: Optional[Any] = None,
    ):
        self.points = np.asarray(points).astype(float)
        self.translation = np.asarray(translation).astype(float)
        self.rgba = rgba

    @property
    def x(self) -> np.ndarray:
        return self.points[:, 0]

    @property
    def y(self) -> np.ndarray:
        return self.points[:, 1]

    @property
    def z(self) -> np.ndarray:
        return self.points[:, 2]

    def centered(self) -> "ObjectModel":
        return self - self.translation

    def rotated(self, pitch: Number, roll: Number, yaw: Number) -> "ObjectModel":
        # Convert angles to radians
        pitch = np.radians(pitch)
        roll = np.radians(roll)
        yaw = np.radians(yaw)

        # Create rotation matrices
        Rx = np.array(
            [
                [1, 0, 0],
                [0, np.cos(pitch), -np.sin(pitch)],
                [0, np.sin(pitch), np.cos(pitch)],
            ]
        )
        Ry = np.array(
            [
                [np.cos(yaw), 0, np.sin(yaw)],
                [0, 1, 0],
                [-np.sin(yaw), 0, np.cos(yaw)],
            ]
        )
        Rz = np.array(
            [
                [np.cos(roll), -np.sin(roll), 0],
                [np.sin(roll), np.cos(roll), 0],
                [0, 0, 1],
            ]
        )
        # Combine rotations
        R = Rz @ Ry @ Rx

        # Undo translation, rotate, and reapply translation.
        centered = self.points - self.translation
        rotated = centered @ R.T
        points = rotated + self.translation

        # Assign points to the new object, and return it.
        out = deepcopy(self)
        out.points = points
        return out

    def __add__(self, translation: ArrayLike) -> "ObjectModel":
        translation = np.asarray(translation)
        out = deepcopy(self)
        out.points += translation
        out.translation += translation
        return out

    def __sub__(self, translation: ArrayLike) -> "ObjectModel":
        translation = np.asarray(translation)
        return self + (-translation)


def load_object_model(
    model_name: str,
    object_name: str,
    checkpoint: Optional[int] = None,
    lm_id: int = 0,
) -> "ObjectModel":
    """Load an object model from a checkpoint.

    Args:
        model_name (str): The name of the model to load.
        object_name (str): The name of the object to load.
        checkpoint (Optional[int]): The checkpoint to load. Defaults to None.
        lm_id (int): The ID of the LM to load. Defaults to 0.

    Returns:
        ObjectModel: The loaded object model.
    """
    if checkpoint is None:
        model_path = DMC_PRETRAIN_DIR / model_name / "pretrained/model.pt"
    else:
        model_path = (
            DMC_PRETRAIN_DIR
            / model_name
            / f"pretrained/checkpoints/{checkpoint}/model.pt"
        )
    data = torch.load(model_path)
    data = data["lm_dict"][lm_id]["graph_memory"][object_name]["patch"]
    points = np.array(data.pos)
    if "rgba" in data.feature_mapping:
        rgba_idx = data.feature_mapping["rgba"]
        rgba = np.array(data.x[:, rgba_idx[0] : rgba_idx[1]]) / 255.0
    else:
        rgba = None
    translation = np.array([0.0, 1.5, 0.0])
    out = ObjectModel(points, translation, rgba)
    return out
