# Copyright 2025 Thousand Brains Project
# Copyright 2023 Numenta Inc.
#
# Copyright may exist in Contributors' modifications
# and/or contributions to the work.
#
# Use of this source code is governed by the MIT
# license that can be found in the LICENSE file or at
# https://opensource.org/licenses/MIT.
"""
Data I/O, paths, and other utilities.
"""

import json
import os
from copy import deepcopy
from pathlib import Path
from typing import Iterable, Mapping, Optional, Union

import numpy as np
import pandas as pd
import torch
from numpy.typing import ArrayLike
from scipy.spatial.transform import Rotation as R

# Path settings - mirrors those in configs/common.py
DMC_ROOT_DIR = Path(os.environ.get("DMC_ROOT_DIR", "~/tbp/results/dmc")).expanduser()
DMC_PRETRAIN_DIR = DMC_ROOT_DIR / "pretrained_models"
DMC_RESULTS_DIR = DMC_ROOT_DIR / "results"
VISUALIZATION_RESULTS_DIR = DMC_ROOT_DIR / "visualizations"

# Root directory for output generated by figure scripts.
DMC_ANALYSIS_DIR = Path(
    os.environ.get("DMC_ANALYSIS_DIR", "~/tbp/results/dmc_analysis")
).expanduser()
DMC_ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)


def load_eval_stats(exp: os.PathLike) -> pd.DataFrame:
    """Load `eval_stats.csv` files.

    This function has 3 main purposes:
     - Load `eval_stats.csv` given just a DMC experiment name since this function
       is aware of DMC result paths.
     - Coerce loaded values into expected types. For example, some columns contain
       arrays, but they're loaded as strings (e.g., "[1.34, 232.33, 123.44]").
     - Add some useful columns to the dataframe (`"episode"`, `"epoch"`).

    Args:
        exp (os.PathLike): Name of a DMC experiment, a directory containing
          `eval_stats.csv`, or a complete path to an `.csv` file.

    Returns:
        pd.DataFrame
    """

    path = Path(exp).expanduser()

    if path.exists():
        # Case 1: Given a path to a csv file.
        if path.suffix.lower() == ".csv":
            df = pd.read_csv(exp, index_col=0)
        # Case 2: Given a path to a directory containing eval_stats.csv.
        elif (path / "eval_stats.csv").exists():
            df = pd.read_csv(path / "eval_stats.csv", index_col=0)
        else:
            raise FileNotFoundError(f"No eval_stats.csv found for {exp}")
    else:
        # Given a run name. Look in DMC folder.
        df = pd.read_csv(DMC_RESULTS_DIR / path / "eval_stats.csv", index_col=0)

    # Collect basic info, like number of LMs, objects, number of episodes, etc.
    n_lms = len(np.unique(df["lm_id"]))
    object_names = np.unique(df["primary_target_object"])
    n_objects = len(object_names)

    # Add 'episode' column.
    assert len(df) % n_lms == 0
    n_episodes = int(len(df) / n_lms)
    df["episode"] = np.repeat(np.arange(n_episodes), n_lms)

    # Add 'epoch' column.
    rows_per_epoch = n_objects * n_lms
    assert len(df) % rows_per_epoch == 0
    n_epochs = int(len(df) / rows_per_epoch)
    df["epoch"] = np.repeat(np.arange(n_epochs), rows_per_epoch)

    # Decode array columns.
    def decode_array_string(s: str, dtype: type = float) -> np.ndarray:
        if not isinstance(s, str):
            return s
        if s in ("", "None"):
            return None
        s = s.strip("[]")
        if "," in s:
            lst = [elt.strip() for elt in s.split(",")]
        else:
            lst = s.split()
        lst = [np.nan if elt == "None" else dtype(elt) for elt in lst]
        return np.array(lst)

    array_cols = [
        "primary_target_position",
        "primary_target_rotation_euler",
        "most_likely_rotation",
        "detected_location",
        "detected_rotation",
        "location_rel_body",
        "detected_path",
        "most_likely_rotation",
        "primary_target_rotation_quat",
    ]
    column_order = list(df.columns)
    for col in array_cols:
        df[col] = df[col].apply(decode_array_string)
    df = df[column_order]
    return df


def get_percent_correct(df: pd.DataFrame) -> float:
    """Get percent of correct object recognition for an `eval_stats` dataframe.

    Uses the 'primary_performance' column. Values 'correct' or 'correct_mlh' count
    as correct.

    """
    n_correct = df.primary_performance.str.startswith("correct").sum()
    return 100 * n_correct / len(df)


def describe_dict(data: Mapping, level: int = 0):
    """
    Recursively describe the contents of a nested dictionary. For visualizing the
    structure of detailed JSON stats. Can be removed when out of data exploration phase.

    Args:
        data (dict): The dictionary to describe.
        level (int): Current depth level in the nested dictionary.
    """
    if not isinstance(data, dict):
        print(f"{'  ' * level}- Not a dictionary: {type(data).__name__}")
        return

    for key in sorted(data.keys()):
        obj = data[key]
        type_ = type(obj).__name__

        if isinstance(obj, dict):
            print(f"{'  ' * level}'{key}': {type_} (len: {len(obj)})")
            # Recursively describe nested dictionaries
            describe_dict(obj, level + 1)

        elif isinstance(obj, (tuple, list)):
            if len(obj) > 0 and isinstance(obj[0], dict):
                print(f"{'  ' * level}'{key}': {type_} of dict (len: {len(obj)})")
                describe_dict(obj[0], level + 1)
            else:
                print(f"{'  ' * level}'{key}': {type_} (len: {len(obj)})")
        else:
            print(f"{'  ' * level}'{key}': {type_}")


class DetailedJSONStatsInterface:
    """Convenience interface to detailed JSON stats.

    This class is a dict-like interface to detailed JSON stats files that loads
    episodes one at a time. An episode can be loaded via `stats[episode_num]`
    (or, equivalently `stats.read_episode(episode_num)`), which takes about
    1.5 - 6.5 seconds per episode. If you plan on loading all episodes eventually,
    the most efficient method is to iterate over a `DetailedJSONStatsInterface`.

    Example:
        >>> stats = DetailedJSONStatsInterface("detailed_stats.json")
        >>> last_episode_data = stats[-1]  # Get data for the last episode.
        >>> # Iterate over all episodes.
        >>> for i, episode_data in enumerate(stats):
        ...     # Process episode data
        ...     pass
    """

    def __init__(self, path: os.PathLike):
        self._path = Path(path)
        self._index = None  # Just used to convert possibly negative indices

    @property
    def path(self) -> os.PathLike:
        return self._path

    def read_episode(self, episode: int) -> Mapping:
        self._check_initialized()
        assert np.isscalar(episode)
        episode = self._index[episode]
        with open(self._path, "r") as f:
            for i, line in enumerate(f):
                if i == episode:
                    return list(json.loads(line).values())[0]

    def _check_initialized(self):
        if self._index is not None:
            return
        length = 0
        with open(self._path, "r") as f:
            length = sum(1 for _ in f)
        self._index = np.arange(length)

    def __iter__(self):
        with open(self._path, "r") as f:
            for i, line in enumerate(f):
                yield list(json.loads(line).values())[0]

    def __len__(self) -> int:
        self._check_initialized()
        return len(self._index)

    def __getitem__(self, episode: int) -> Mapping:
        """Get the stats for a given episode.

        Args:
            episode (int): The episode number.

        Returns:
            Mapping: The stats for the episode.
        """
        return self.read_episode(episode)


class ObjectModel:
    """Mutable wrapper for object models.

    Args:
        points (ArrayLike): The points of the object model as a sequence of points
          (i.e., has shape (n_points, 3)).
        features (Optional[Mapping]): The features of the object model. For
          convenience, the features become attributes of the ObjectModel instance.
    """
    def __init__(
        self,
        points: ArrayLike,
        features: Optional[Mapping[str, ArrayLike]] = None,
    ):
        self.points = np.asarray(points, dtype=float)
        if features:
            for key, value in features.items():
                setattr(self, key, np.asarray(value))

    @property
    def x(self) -> np.ndarray:
        return self.points[:, 0]

    @property
    def y(self) -> np.ndarray:
        return self.points[:, 1]

    @property
    def z(self) -> np.ndarray:
        return self.points[:, 2]

    @property
    def pos(self) -> np.ndarray:
        """Alias for `points` for compatibility with `GraphObjectModel` objects."""
        return self.points

    @pos.setter
    def pos(self, arr: ArrayLike):
        assert arr.shape == self.points.shape
        self.points = np.asarray(arr)

    def copy(self, deep: bool = True) -> "ObjectModel":
        return deepcopy(self) if deep else self

    def centered(self, method: str = "bbox") -> "ObjectModel":
        return self - self.get_center(method)

    def get_center(self, method: str = "bbox"):
        return get_center(self.points, method)

    def rotated(
        self,
        rotation: Union[R, ArrayLike],
        degrees: bool = False,
    ) -> "ObjectModel":
        """Rotate the object model.

        Args:
            rotation: Rotation to apply. May be one of
              - A `scipy.spatial.transform.Rotation` object.
              - A 3x3 rotation matrix.
              - A 3-element array of x, y, z euler angles.
            degrees (bool): Whether Euler angles are in degrees. Ignored
                if `rotation` is not a 1D array.

        Returns:
            ObjectModel: The rotated object model.
        """
        if isinstance(rotation, R):
            rot = rotation
        else:
            arr = np.asarray(rotation)
            if arr.shape == (3,):
                rot = R.from_euler("xyz", arr, degrees=degrees)
            elif arr.shape == (3, 3):
                rot = R.from_matrix(arr)
            else:
                raise ValueError(f"Invalid rotation argument: {rotation}")

        points = rot.apply(self.points)
        out = self.copy()
        out.points = points

        return out

    def __add__(self, translation: ArrayLike) -> "ObjectModel":
        translation = np.asarray(translation)
        out = deepcopy(self)
        out.points += translation
        return out

    def __sub__(self, translation: ArrayLike) -> "ObjectModel":
        translation = np.asarray(translation)
        return self + (-translation)


def get_center(points: ArrayLike, method: str = "bbox") -> np.ndarray:
    """Get the center of a set of points.

    Args:
        points (ArrayLike): The points to get the center of.
        method (str): The method to use to get the center. One of
            - "mean": The mean of the points.
            - "bbox": The center of the bounding box of the points.
    """
    points = np.asarray(points)
    if method == "mean":
        return np.mean(points, axis=0)
    elif method == "bbox":
        return (np.min(points, axis=0) + np.max(points, axis=0)) / 2
    else:
        raise ValueError(f"Invalid method: {method}")


def load_object_model(
    model_name: str,
    object_name: str,
    features: Optional[Iterable[str]] = ("rgba",),
    checkpoint: Optional[int] = None,
    lm_id: int = 0,
) -> ObjectModel:
    """Load an object model from a pretraining experiment.

    Args:
        model_name (str): The name of the model to load (e.g., `dist_agent_1lm`).
        object_name (str): The name of the object to load (e.g., `mug`).
        checkpoint (Optional[int]): The checkpoint to load. Defaults to None. Most
          pretraining experiments aren't checkpointed, so this is usually None.
        lm_id (int): The ID of the LM to load. Defaults to 0.

    Returns:
        ObjectModel: The loaded object model.

    Example:
        >>> model = load_object_model("dist_agent_1lm", "mug")
        >>> model -= [0, 1.5, 0]
        >>> rotation = R.from_euler("xyz", [0, 90, 0], degrees=True)
        >>> rotated = model.rotated(rotation)
        >>> print(model.rgba.shape)
        (1354, 4)
    """
    if checkpoint is None:
        model_path = DMC_PRETRAIN_DIR / model_name / "pretrained/model.pt"
    else:
        model_path = (
            DMC_PRETRAIN_DIR
            / model_name
            / f"pretrained/checkpoints/{checkpoint}/model.pt"
        )
    data = torch.load(model_path)
    data = data["lm_dict"][lm_id]["graph_memory"][object_name]["patch"]
    points = np.array(data.pos, dtype=float)
    if features:
        features = [features] if isinstance(features, str) else features
        feature_dict = {}
        for feature in features:
            if feature not in data.feature_mapping:
                print(f"WARNING: Feature {feature} not found in data.feature_mapping")
                continue
            idx = data.feature_mapping[feature]
            feature_data = np.array(data.x[:, idx[0] : idx[1]])
            if feature == "rgba":
                feature_data = feature_data / 255.0
            feature_dict[feature] = feature_data

    return ObjectModel(points, features=feature_dict)
