# Copyright 2025 Thousand Brains Project
# Copyright 2023 Numenta Inc.
#
# Copyright may exist in Contributors' modifications
# and/or contributions to the work.
#
# Use of this source code is governed by the MIT
# license that can be found in the LICENSE file or at
# https://opensource.org/licenses/MIT.
"""
Data I/O, filesystem, and other utilities.
"""

import json
import os
from copy import deepcopy
from numbers import Number
from pathlib import Path
from typing import Any, Iterable, Mapping, Optional, Union

import numpy as np
import pandas as pd
import scipy
import torch
from numpy.typing import ArrayLike
from scipy.spatial.transform import Rotation as R

# Path settings - mirrors those in configs/common.py
DMC_ROOT_DIR = Path(os.environ.get("DMC_ROOT_DIR", "~/tbp/results/dmc")).expanduser()
DMC_PRETRAIN_DIR = DMC_ROOT_DIR / "pretrained_models"
DMC_RESULTS_DIR = DMC_ROOT_DIR / "results"
VISUALIZATION_RESULTS_DIR = DMC_ROOT_DIR / "visualizations"

# Root directory for output generated by figure scripts.
DMC_ANALYSIS_DIR = Path(
    os.environ.get("DMC_ANALYSIS_DIR", "~/tbp/results/dmc_analysis")
).expanduser()
DMC_ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)


def load_eval_stats(exp: os.PathLike) -> pd.DataFrame:
    """Load `eval_stats.csv`

    Convenience function for loading `eval_stats.csv` from a DMC experiment name.

    Args:
        exp (os.PathLike): Name of a DMC experiment, a directory containing
        `eval_stats.csv`, or a complete path to `eval_stats.csv`.

    Returns:
        pd.DataFrame: The loaded dataframe. Includes generated columns `episode` and
        `epoch`.
    """

    path = Path(exp).expanduser()

    if path.exists():
        # Case 1: Given a path to a csv file.
        if path.suffix.lower() == ".csv":
            df = pd.read_csv(exp, index_col=0)
        # Case 2: Given a path to a directory containing eval_stats.csv.
        elif (path / "eval_stats.csv").exists():
            df = pd.read_csv(path / "eval_stats.csv", index_col=0)
        else:
            raise FileNotFoundError(f"No eval_stats.csv found for {exp}")
    else:
        # Given a run name. Look in DMC folder.
        df = pd.read_csv(DMC_RESULTS_DIR / path / "eval_stats.csv", index_col=0)

    # Collect basic info, like number of LMs, objects, number of episodes, etc.
    n_lms = len(np.unique(df["lm_id"]))
    object_names = np.unique(df["primary_target_object"])
    n_objects = len(object_names)

    # Add 'episode' column.
    assert len(df) % n_lms == 0
    n_episodes = int(len(df) / n_lms)
    df["episode"] = np.repeat(np.arange(n_episodes), n_lms)

    # Add 'epoch' column.
    rows_per_epoch = n_objects * n_lms
    assert len(df) % rows_per_epoch == 0
    n_epochs = int(len(df) / rows_per_epoch)
    df["epoch"] = np.repeat(np.arange(n_epochs), rows_per_epoch)

    # Decode array columns.
    def decode_array_string(s: str, dtype: type = float) -> np.ndarray:
        if not isinstance(s, str):
            return s
        if s in ("", "None"):
            return None
        s = s.strip("[]")
        if "," in s:
            lst = [elt.strip() for elt in s.split(",")]
        else:
            lst = s.split()
        lst = [np.nan if elt == "None" else dtype(elt) for elt in lst]
        return np.array(lst)

    array_cols = [
        "primary_target_position",
        "primary_target_rotation_euler",
        "most_likely_rotation",
        "detected_location",
        "detected_rotation",
        "location_rel_body",
        "detected_path",
        "most_likely_rotation",
        "primary_target_rotation_quat",
    ]
    column_order = list(df.columns)
    for col in array_cols:
        df[col] = df[col].apply(decode_array_string)
    df = df[column_order]
    return df


def get_percent_correct(df: pd.DataFrame) -> float:
    """Get percent of correct object recognition for an `eval_stats` dataframe.

    Uses the 'primary_performance' column. Values 'correct' or 'correct_mlh' count
    as correct.

    """
    n_correct = df.primary_performance.str.startswith("correct").sum()
    return 100 * n_correct / len(df)


def describe_dict(data: Mapping, level: int = 0):
    """
    Recursively describe the contents of a nested dictionary. For visualizing the
    structure of detailed JSON stats. Can be removed when out of data exploration phase.

    Args:
        data (dict): The dictionary to describe.
        level (int): Current depth level in the nested dictionary.
    """
    if not isinstance(data, dict):
        print(f"{'  ' * level}- Not a dictionary: {type(data).__name__}")
        return

    for key in sorted(data.keys()):
        obj = data[key]
        type_ = type(obj).__name__

        if isinstance(obj, dict):
            print(f"{'  ' * level}'{key}': {type_} (len: {len(obj)})")
            # Recursively describe nested dictionaries
            describe_dict(obj, level + 1)

        elif isinstance(obj, (tuple, list)):
            if len(obj) > 0 and isinstance(obj[0], dict):
                print(f"{'  ' * level}'{key}': {type_} of dict (len: {len(obj)})")
                describe_dict(obj[0], level + 1)
            else:
                print(f"{'  ' * level}'{key}': {type_} (len: {len(obj)})")
        else:
            print(f"{'  ' * level}'{key}': {type_}")


class DetailedJSONStatsInterface:
    """Convenience interface to detailed JSON stats.

    This class is a dict-like interface to detailed JSON stats files that loads
    episodes one at a time. An episode can be loaded via `stats[episode_num]`
    (or, equivalently `stats.read_episode(episode_num)`), which takes about
    1.5 - 6.5 seconds per episode. If you plan on loading all episodes eventually,
    the most efficient method is to iterate over a `DetailedJSONStatsInterface`.

    Example:
        >>> stats = DetailedJSONStatsInterface("detailed_stats.json")
        >>> last_episode_data = stats[-1]  # Get data for the last episode.
        >>> # Iterate over all episodes.
        >>> for i, episode_data in enumerate(stats):
        ...     # Process episode data
        ...     pass
    """

    def __init__(self, path: os.PathLike):
        self._path = Path(path)
        self._index = None  # Just used to convert possibly negative indices

    @property
    def path(self) -> os.PathLike:
        return self._path

    def read_episode(self, episode: int) -> Mapping:
        self._check_initialized()
        assert np.isscalar(episode)
        episode = self._index[episode]
        with open(self._path, "r") as f:
            for i, line in enumerate(f):
                if i == episode:
                    return json.loads(line)[str(i)]

    def _check_initialized(self):
        if self._index is not None:
            return
        length = 0
        with open(self._path, "r") as f:
            length = sum(1 for _ in f)
        self._index = np.arange(length)

    def __iter__(self):
        with open(self._path, "r") as f:
            for i, line in enumerate(f):
                yield json.loads(line)[str(i)]

    def __len__(self) -> int:
        self._check_initialized()
        return len(self._index)

    def __getitem__(self, episode: int) -> Mapping:
        """Get the stats for a given episode.

        Args:
            episode (int): The episode number.

        Returns:
            Mapping: The stats for the episode.
        """
        return self.read_episode(episode)


class ObjectModel:
    def __init__(
        self,
        points: ArrayLike,
        features: Optional[Mapping[str, ArrayLike]] = None,
    ):
        self.points = np.asarray(points, dtype=float)
        if features:
            for key, value in features.items():
                setattr(self, key, np.asarray(value))

    @property
    def x(self) -> np.ndarray:
        return self.points[:, 0]

    @property
    def y(self) -> np.ndarray:
        return self.points[:, 1]

    @property
    def z(self) -> np.ndarray:
        return self.points[:, 2]

    @property
    def pos(self) -> np.ndarray:
        return self.points

    @pos.setter
    def pos(self, arr: ArrayLike):
        assert arr.shape == self.points.shape
        self.points = np.asarray(arr)

    def copy(self, deep: bool = True) -> "ObjectModel":
        return deepcopy(self) if deep else self

    def centered(self, method: str = "bbox") -> "ObjectModel":
        return self - self.get_center(method)

    def get_center(self, method: str = "bbox"):
        return get_center(self.points, method)

    def rotated(
        self,
        rotation: Union[R, ArrayLike],
        degrees: bool = False,
        world: bool = False,
    ) -> "ObjectModel":
        """Rotate the object model.

        Args:
            rotation (Union[R, ArrayLike]): Rotation to apply. May be
              - A `scipy.spatial.transform.Rotation` object.
              - A 3x3 rotation matrix.
              - A 3-element array of x, y, z euler angles.
            degrees (bool, optional): Whether Euler angles are in degrees. Ignored
                if `rotation` is not a 1D array.
            world (bool, optional): If True, rotate the object in world coordinates.
                If False, rotate the object about its own center.

        Returns:
            ObjectModel: _description_
        """
        if isinstance(rotation, R):
            rot = rotation
        else:
            arr = np.asarray(rotation)
            if arr.shape == (3,):
                rot = R.from_euler("xyz", arr, degrees=degrees)
            elif arr.shape == (3, 3):
                rot = R.from_matrix(arr)
            else:
                raise ValueError(f"Invalid rotation argument: {rotation}")

        if world:
            center = self.get_center()
            points = rot.apply(self.points - center)
            points = points + center
        else:
            points = rot.apply(self.points)

        out = self.copy()
        out.points = points

        return out

    # def rotated(self, pitch: Number, roll: Number, yaw: Number) -> "ObjectModel":
    #     # Convert angles to radians
    #     pitch = np.radians(pitch)
    #     roll = np.radians(roll)
    #     yaw = np.radians(yaw)

    #     # Create rotation matrices
    #     Rx = np.array(
    #         [
    #             [1, 0, 0],
    #             [0, np.cos(pitch), -np.sin(pitch)],
    #             [0, np.sin(pitch), np.cos(pitch)],
    #         ]
    #     )
    #     Ry = np.array(
    #         [
    #             [np.cos(yaw), 0, np.sin(yaw)],
    #             [0, 1, 0],
    #             [-np.sin(yaw), 0, np.cos(yaw)],
    #         ]
    #     )
    #     Rz = np.array(
    #         [
    #             [np.cos(roll), -np.sin(roll), 0],
    #             [np.sin(roll), np.cos(roll), 0],
    #             [0, 0, 1],
    #         ]
    #     )
    #     # Combine rotations
    #     R = Rz @ Ry @ Rx

    #     # Undo translation, rotate, and reapply translation.
    #     centered = self.points - self.translation
    #     rotated = centered @ R.T
    #     points = rotated + self.translation

    #     # Assign points to the new object, and return it.
    #     out = deepcopy(self)
    #     out.points = points
    #     return out

    def __add__(self, translation: ArrayLike) -> "ObjectModel":
        translation = np.asarray(translation)
        out = deepcopy(self)
        out.points += translation
        return out

    def __sub__(self, translation: ArrayLike) -> "ObjectModel":
        translation = np.asarray(translation)
        return self + (-translation)


def load_object_model(
    model_name: str,
    object_name: str,
    features: Optional[Iterable[str]] = ("rgba",),
    checkpoint: Optional[int] = None,
    lm_id: int = 0,
) -> "ObjectModel":
    """Load an object model from a checkpoint.

    Args:
        model_name (str): The name of the model to load.
        object_name (str): The name of the object to load.
        checkpoint (Optional[int]): The checkpoint to load. Defaults to None.
        lm_id (int): The ID of the LM to load. Defaults to 0.

    Returns:
        ObjectModel: The loaded object model.
    """
    if checkpoint is None:
        model_path = DMC_PRETRAIN_DIR / model_name / "pretrained/model.pt"
    else:
        model_path = (
            DMC_PRETRAIN_DIR
            / model_name
            / f"pretrained/checkpoints/{checkpoint}/model.pt"
        )
    data = torch.load(model_path)
    data = data["lm_dict"][lm_id]["graph_memory"][object_name]["patch"]
    points = np.array(data.pos, dtype=float)
    if features:
        features = [features] if isinstance(features, str) else features
        feature_dict = {}
        for feature in features:
            if feature not in data.feature_mapping:
                print(f"WARNING: Feature {feature} not found in data.feature_mapping")
                continue
            idx = data.feature_mapping[feature]
            feature_data = np.array(data.x[:, idx[0] : idx[1]])
            if feature == "rgba":
                feature_data = feature_data / 255.0
            feature_dict[feature] = feature_data

    out = ObjectModel(points, features=feature_dict)
    return out


def get_center(points: ArrayLike, method: str = "bbox") -> np.ndarray:
    """Get the center of a set of points.

    Args:
        points (ArrayLike): The points to get the center of.
        method (str): The method to use to get the center. One of
            - "centroid": The mean of the points.
            - "bbox": The center of the bounding box of the points.
    """
    points = np.asarray(points)
    if method == "mean":
        return np.mean(points, axis=0)
    elif method == "bbox":
        return (np.min(points, axis=0) + np.max(points, axis=0)) / 2
    else:
        raise ValueError(f"Invalid method: {method}")
